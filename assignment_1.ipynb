{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "assignment_1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zzhu247/CS4414/blob/main/assignment_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UAF2tNSPZc0j"
      },
      "source": [
        "# Grade: /100 points\n",
        "\n",
        "# Assignment 01: Supervised learning, Linear models, and Loss functions\n",
        "\n",
        "In this assignment, you're going to write your own methods to fit a linear model using either an OLS or LAD cost function.  \n",
        "\n",
        "## Data set \n",
        "\n",
        "For this assignment, we will examine some data representing possums in Australia and New Guinea. The data frame contains 46 observations on the following 6 variables:\n",
        "\n",
        "* sex: Sex, either m (male) or f (female).\n",
        "* age: Age in years.\n",
        "* headL: Head length, in mm.\n",
        "* skullW: Skull width, in mm.\n",
        "* totalL: Total length, in cm.\n",
        "* tailL: Tail length, in cm.\n",
        "\n",
        "## Follow These Steps Before Submitting\n",
        "Once you are finished, ensure to complete the following steps.\n",
        "\n",
        "1.  Restart your kernel by clicking 'Kernel' > 'Restart & Run All'.\n",
        "\n",
        "2.  Fix any errors which result from this.\n",
        "\n",
        "3.  Repeat steps 1. and 2. until your notebook runs without errors.\n",
        "\n",
        "4.  Submit your completed notebook to OWL by the deadline.\n",
        "\n",
        "\n",
        "## Preliminaries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XkvrPaWzZc0k"
      },
      "source": [
        "# Import all the necessary packages: \n",
        "import matplotlib.pyplot as plt \n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy.stats as ss \n",
        "import scipy.optimize as so\n",
        "from sklearn import linear_model\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RjAh_7PPZc0l"
      },
      "source": [
        "\n",
        "## Part 1\n",
        "### Question 1.1:  /10 points\n",
        "\n",
        "\n",
        "Read in the `possum.csv` file as a `pandas.DataFrame`.  Investigate the relationship between the possum's age and its tail length by plotting a scatter plot of the `age` and `tailL` columns. Add an `alpha`(transparency of the plotted dots) in case some data are overlapping. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "Mwh6nqghZc0l",
        "outputId": "5b08a550-694a-4409-81bf-6d5a01ea8669"
      },
      "source": [
        "# Read in the data with pandas\n",
        "possum_data = pd.read_csv(\"possum.csv\")\n",
        "possum_data.plot.scatter(x = 'age', y = 'tailL', alpha = 0.5)\n",
        "plt.show()\n",
        "\n",
        "# Make the scatter plot (don't forget the axis labels)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NcF4SMpXZc0l"
      },
      "source": [
        "### Question 1.2: /5 point\n",
        "\n",
        "Recall that the linear model, we obtain predictions by computing \n",
        "\n",
        "$$ \\hat{\\mathbf{y}} = \\mathbf{X} \\hat{\\beta} $$\n",
        "\n",
        "Here, $\\mathbf{X}$ is a design matrix which includes a column of ones, $\\hat{\\beta}$ are coefficients, and $\\hat{\\mathbf{y}}$ are outcomes.  Write a function `linearModelPredict` to compute linear model predictions given data and a coefficient vector.  The function should take as it's arguments a 1d-array of coefficients `b` and the design matrix `X` as a 2d-array and return linear model predictions `yp`.\n",
        "\n",
        "Test the function by setting \n",
        "\n",
        "```\n",
        "X = np.array([[1,0],[1,-1],[1,2]])\n",
        "b = np.array([0.1,0.3])\n",
        "```\n",
        "and call your function with these values! \n",
        "\n",
        "Report $\\hat{\\mathbf{y}}$. \n",
        "What is the dimensionality of the numpy-array that you get back? \n",
        "\n",
        "Hint:  Read the documentation for `np.dot` or the `@` operator in `numpy`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_YDTe7ZTZc0m"
      },
      "source": [
        "def linearModelPredict(b,X):\n",
        "    yp = np.dot(X, b.T)\n",
        "    return yp\n",
        "# Always important: Test the new function you have written! \n",
        "\n",
        "# By the way: What happens when b is a 2d-array? \n",
        "\n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-msrHZ3lPyC"
      },
      "source": [
        "If both X and b are 2-D arrays, it is matrix multiplication, but using matmul or a @ b is preferred."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L5CFqY0yZc0m"
      },
      "source": [
        "### Question 1.3: /15 points\n",
        "\n",
        "Write a function `linearModelLossRSS` which computes and returns the loss function for an OLS model parameterized by $\\beta$, as well as the gradient of the loss.  The function should take as its first argument a 1d-array `beta` of coefficients for the linear model, as its second argument the design matrix `X` as a 2d-array, and as its third argument a 1d-array `y` of observed outcomes.\n",
        "\n",
        "Test the function with the values \n",
        "\n",
        "```\n",
        "X = np.array([[1,0],[1,-1],[1,2]])\n",
        "b = np.array([0.1,0.3])\n",
        "y = np.array([0,0.4,2]) \n",
        "```\n",
        "\n",
        "Report the loss and the gradient. \n",
        "\n",
        "**Written answer**: To minimize the cost do you need increase or decrease the value of the parameters? "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MMzSL4qaZc0m",
        "outputId": "8d2dfa8c-a6d8-4667-e892-c326f4d518c5"
      },
      "source": [
        "X = np.array([[1,0],[1,-1],[1,2]])\n",
        "b = np.array([0.1,0.3])\n",
        "y = np.array([0,0.4,2])\n",
        "def linearModelLossRSS(b,X,y):\n",
        "  #get y.cap which is our predicted y\n",
        "  predy = linearModelPredict(b, X)\n",
        "  #get the vactor of residuals\n",
        "  res = y - predy\n",
        "  #Get the residuals sums of square\n",
        "  residual_sum_of_squares = np.sum(res**2)\n",
        "  #get the gradiant\n",
        "  gradient =(-2)*X*residual_sum_of_squares\n",
        "  return (residual_sum_of_squares, gradient)\n",
        "print(linearModelLossRSS(b,X,y))\n",
        "\n"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2.06, array([[-4.12,  0.  ],\n",
            "       [-4.12,  4.12],\n",
            "       [-4.12, -8.24]]))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ECn1R3qjZc0m"
      },
      "source": [
        "### Question 1.4:  /15 points. \n",
        "\n",
        "Now that you've implemented a loss function in question 1.3, it is now time to minimize it!\n",
        "\n",
        "Write a function `linearModelFit` to fit a linear model.  The function should take as its first argument the design matrix `X` as a 2d-array, as its second argument a 1d-array `y` of outcomes, and as its third argument a function  `lossfcn` which returns as a tuple the value of the loss, as well as the gradient of the loss. As a result, it should return the estimated betas and the R2. \n",
        "\n",
        "Test the function with the values: \n",
        "```\n",
        "X = np.array([[1,0],[1,-1],[1,2]])\n",
        "y = np.array([0,0.4,2]) \n",
        "```\n",
        "\n",
        "Report best parameters and the fitted R2 \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341
        },
        "id": "i0J26vKOZc0n",
        "outputId": "64aa995a-b5c4-42d1-8726-7352ee77d9a7"
      },
      "source": [
        "def linearModelFit(X,y,lossfcn = linearModelLossRSS):\n",
        "  result = so.minimize(linearModelLossRSS(b,X,y),[1,0],args = (X, y), jac = True)\n",
        "  esitimated_beta = result.x\n",
        "  R2 = 1 - ((linearModelLossRSS(b,X,y))/(np.sum((y - np.average(y))**2)))\n",
        "  return (estimated_betas,R2)\n",
        "\n",
        "X = np.array([[1,0],[1,-1],[1,2]])\n",
        "b = np.array([0.1,0.3])\n",
        "y = np.array([0,0.4,2])\n",
        "print(linearModelFit(X,y,lossfcn = linearModelLossRSS))"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-41-44fa69001356>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinearModelFit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlossfcn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinearModelLossRSS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-41-44fa69001356>\u001b[0m in \u001b[0;36mlinearModelFit\u001b[0;34m(X, y, lossfcn)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mlinearModelFit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlossfcn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinearModelLossRSS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mso\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinearModelLossRSS\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjac\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m   \u001b[0mesitimated_beta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mR2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinearModelLossRSS\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maverage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mestimated_betas\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mR2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/scipy/optimize/_minimize.py\u001b[0m in \u001b[0;36mminimize\u001b[0;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[1;32m    602\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_minimize_cg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjac\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    603\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'bfgs'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 604\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_minimize_bfgs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjac\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    605\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'newton-cg'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    606\u001b[0m         return _minimize_newtoncg(fun, x0, args, jac, hess, hessp, callback,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/scipy/optimize/optimize.py\u001b[0m in \u001b[0;36m_minimize_bfgs\u001b[0;34m(fun, x0, args, jac, callback, gtol, norm, eps, maxiter, disp, return_all, **unknown_options)\u001b[0m\n\u001b[1;32m   1001\u001b[0m     \u001b[0mfunc_calls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwrap_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1002\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1003\u001b[0;31m     \u001b[0mold_fval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1004\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1005\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfprime\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/scipy/optimize/optimize.py\u001b[0m in \u001b[0;36mfunction_wrapper\u001b[0;34m(*wrapper_args)\u001b[0m\n\u001b[1;32m    325\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfunction_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mwrapper_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m         \u001b[0mncalls\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 327\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapper_args\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mncalls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunction_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/scipy/optimize/optimize.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, x, *args)\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0mfg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjac\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'tuple' object is not callable"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6FA8O9rCZc0n"
      },
      "source": [
        "### Question 1.5: /15 points\n",
        "\n",
        "Use the above functions to fit your model to the possum data. Then use your model and the fitted parameters to make predictions along a grid of equally spaced possum ages.  \n",
        "\n",
        "Plot the data and add a line for the predicted values. You can get these by generating a new X-matrix with equally space ages (using for example np.linspace). Also report the R2 value for the fit. You can do this by either printing out the R2 of the fit or putting it on your plot via the `annotate` function in matplotlib.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5KgqHsFrZc0n"
      },
      "source": [
        "# Make the design matrix using np.c_ \n",
        "# y = ...\n",
        "# X =  np.c_[...]\n",
        "# Call your fitting function \n",
        "\n",
        "# Create the scatter plot (see question 1.1)\n",
        "\n",
        "# Create a new X matrix with equally space data \n",
        "\n",
        "# Add the line to the graph \n",
        "\n",
        "# Report R2 "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y8lNCPhHZc0n"
      },
      "source": [
        "## Part 2: LAD Regression\n",
        "\n",
        "### Question 2.1:  /15 points\n",
        "\n",
        "In the previous section, we worked with the squared loss.  Now, we'll implement a linear model with least absolute deviation loss.\n",
        "\n",
        "Write a function `linearModelLossLAD` which computes the least absolute deviation loss function for a linear model  parameterized by $\\beta$, as well as the gradient of the loss.  The function should take as its first argument a 1d-array `beta` of coefficients for the linear model, as its second argument the design matrix `X` as a 2d-array, and as its third argument a 1d-array `y` of observed outcomes.\n",
        "\n",
        "Test the function with the values \n",
        "\n",
        "```\n",
        "X = np.array([[1,0],[1,-1],[1,2]])\n",
        "b = np.array([0.1,0.3])\n",
        "y = np.array([0,0.4,2]) \n",
        "```\n",
        "\n",
        "Report the loss and the gradient. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4YycCBz-Zc0o"
      },
      "source": [
        "def linearModelLossLAD(b,X,y):\n",
        "\n",
        "    return (sum_abs_dev,grad)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m-TJD3tXZc0o"
      },
      "source": [
        "### Question 2.2: /10 points\n",
        "\n",
        "\n",
        "Use the above functions to fit your LAD model. Use your model to make predictions along a grid of equally spaced possum ages.  Once fit, add the fitted line to the scatter plot as in question 1.5.  Also report the R2-value. \n",
        "\n",
        "**Written answer**: What is the difference in the fit obtained with an L1 as compared to the L2 cost function? Which one has a higher R2 value? Why?  \n",
        "\n",
        "Note: If you recieve an error from the optimizer, it may be because the loss function for the LAD model is not differentiable at its minimum.  This will lead to some gradient based optimizers to fail to converge.  If this happens to you then pass `method=\"Powell\"` to `scipy.optimize.minimize`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FmTmEYeFZc0o"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wJF3gTh_Zc0p"
      },
      "source": [
        "Written answer: The LAD fit does not give as much weight to the outlier (9,55) as the OLS fit. The R2 value is lower, however. This is because OLS minimized the RSS, and therefore maximizes R2.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zFwqZ22NZc0p"
      },
      "source": [
        "### Question 2.3: /15 points\n",
        "\n",
        "Fit an OLS model to the possum data with the `linear_model` module from the `sklearn` package by using the `LinearRegression` class.  In no more than two sentences, comment on the rsquared values from `sklearn` and the rsquared values from your models. Are they similar?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ZAhKIFXZc0p"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vqBxeYmiZc0p"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}